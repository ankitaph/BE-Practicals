{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a19e781",
   "metadata": {},
   "source": [
    "1. Write a program for pre-processing of a text document such as stop word removal, stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b71d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ankita\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\ankita\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\ankita\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ankita\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ankita\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ankita\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3ec5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "711aab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ANKITA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f0f533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'processing', 'encompasses', 'a', 'series', 'of', 'operations', 'that', 'convert', 'raw', 'data', 'into', 'structured', 'and', 'organized', 'information', '.', 'This', 'process', 'begins', 'with', 'data', 'collection', ',', 'where', 'data', 'is', 'gathered', 'from', 'various', 'sources', 'such', 'as', 'sensors', ',', 'databases', ',', 'forms', ',', 'or', 'external', 'systems', '.', 'Once', 'collected', ',', 'the', 'data', 'can', 'be', 'in', 'various', 'formats', ',', 'including', 'text', ',', 'numbers', ',', 'images', ',', 'or', 'multimedia', '.', 'The', 'next', 'step', 'in', 'data', 'processing', 'is', 'data', 'cleaning', 'and', 'validation', '.', 'This', 'involves', 'identifying', 'and', 'correcting', 'errors', ',', 'inconsistencies', ',', 'and', 'missing', 'values', 'in', 'the', 'data', '.', 'Clean', 'and', 'accurate', 'data', 'is', 'essential', 'for', 'reliable', 'analysis', 'and', 'decision-making', '.', 'Data', 'cleaning', 'often', 'involves', 'techniques', 'like', 'outlier', 'detection', 'and', 'data', 'imputation', '.', 'After', 'data', 'cleaning', ',', 'data', 'transformation', 'is', 'performed', '.', 'This', 'includes', 'tasks', 'like', 'data', 'normalization', ',', 'aggregation', ',', 'and', 'summarization', '.', 'Normalization', 'ensures', 'that', 'data', 'is', 'on', 'a', 'consistent', 'scale', ',', 'while', 'aggregation', 'and', 'summarization', 'reduce', 'data', 'complexity', 'by', 'generating', 'statistics', 'or', 'aggregating', 'data', 'into', 'meaningful', 'groups', '.', 'Data', 'processing', 'also', 'includes', 'data', 'integration', ',', 'where', 'data', 'from', 'multiple', 'sources', 'is', 'combined', 'into', 'a', 'unified', 'dataset', '.', 'Integration', 'can', 'be', 'challenging', 'due', 'to', 'differences', 'in', 'data', 'structures', 'and', 'formats', '.', 'Techniques', 'like', 'data', 'mapping', 'and', 'data', 'warehousing', 'are', 'used', 'to', 'facilitate', 'integration', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization: This involves breaking up the text into individual tokens,\n",
    "#which are typically words or punctuation marks.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "document = \"\"\"Data processing encompasses a series of operations that convert raw data into structured\n",
    "            and organized information. This process begins with data collection, where data is gathered\n",
    "            from various sources such as sensors, databases, forms, or external systems. Once collected,\n",
    "            the data can be in various formats, including text, numbers, images, or multimedia.\n",
    "            The next step in data processing is data cleaning and validation. This involves identifying\n",
    "            and correcting errors, inconsistencies, and missing values in the data. Clean and accurate data\n",
    "            is essential for reliable analysis and decision-making. Data cleaning often involves techniques\n",
    "            like outlier detection and data imputation.\n",
    "            After data cleaning, data transformation is performed. This includes tasks like data normalization,\n",
    "            aggregation, and summarization. Normalization ensures that data is on a consistent scale, while\n",
    "            aggregation and summarization reduce data complexity by generating statistics or aggregating data into meaningful groups.\n",
    "            Data processing also includes data integration, where data from multiple sources is combined\n",
    "            into a unified dataset. Integration can be challenging due to differences in data structures and\n",
    "            formats. Techniques like data mapping and data warehousing are used to facilitate integration.\"\"\"\n",
    "\n",
    "words = word_tokenize(document)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b32fc3",
   "metadata": {},
   "source": [
    "Stopword Removal and Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1bce20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Data processing encompasses a series of operations that convert raw data into structured\n",
      "            and organized information. This process begins with data collection, where data is gathered\n",
      "            from various sources such as sensors, databases, forms, or external systems. Once collected,\n",
      "            the data can be in various formats, including text, numbers, images, or multimedia.\n",
      "            The next step in data processing is data cleaning and validation. This involves identifying\n",
      "            and correcting errors, inconsistencies, and missing values in the data. Clean and accurate data\n",
      "            is essential for reliable analysis and decision-making. Data cleaning often involves techniques\n",
      "            like outlier detection and data imputation.\n",
      "            After data cleaning, data transformation is performed. This includes tasks like data normalization,\n",
      "            aggregation, and summarization. Normalization ensures that data is on a consistent scale, while\n",
      "            aggregation and summarization reduce data complexity by generating statistics or aggregating data into meaningful groups.\n",
      "            Data processing also includes data integration, where data from multiple sources is combined\n",
      "            into a unified dataset. Integration can be challenging due to differences in data structures and\n",
      "            formats. Techniques like data mapping and data warehousing are used to facilitate integration.\n",
      "\n",
      "Preprocessed Text:\n",
      "data process encompass seri oper convert raw data structur organ inform  process begin data collect  data gather variou sourc sensor  databas  form  extern system  collect  data variou format  includ text  number  imag  multimedia  next step data process data clean valid  involv identifi correct error  inconsist  miss valu data  clean accur data essenti reliabl analysi decision-mak  data clean often involv techniqu like outlier detect data imput  data clean  data transform perform  includ task like data normal  aggreg  summar  normal ensur data consist scale  aggreg summar reduc data complex gener statist aggreg data meaning group  data process also includ data integr  data multipl sourc combin unifi dataset  integr challeng due differ data structur format  techniqu like data map data wareh use facilit integr \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ANKITA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Stop Words Removal: This involves removing commonly occurring words such as \"the\", \"and\", \"a\", etc. \n",
    "#that do not contribute much to the meaning of the text.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the English stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "\n",
    "#Stemming: This involves reducing words to their base or root form.\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the NLTK Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a list to store the preprocessed words\n",
    "preprocessed_words = []\n",
    "\n",
    "# Perform text preprocessing\n",
    "for word in words:\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    word = word.lower()\n",
    "    word = word.strip('.,?!-()[]{}\"\\'')\n",
    "\n",
    "    # Check if the word is not a stop word\n",
    "    if word not in stop_words:\n",
    "        # Stem the word\n",
    "        word = stemmer.stem(word)\n",
    "\n",
    "        # Add the preprocessed word to the list\n",
    "        preprocessed_words.append(word)\n",
    "\n",
    "# Join the preprocessed words back into a text\n",
    "preprocessed_text = \" \".join(preprocessed_words)\n",
    "\n",
    "# Print the original text and preprocessed text\n",
    "print(\"Original Text:\")\n",
    "print(document)\n",
    "print(\"\\nPreprocessed Text:\")\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b4a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efb8da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
