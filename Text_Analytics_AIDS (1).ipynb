{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aQQL2Qyd7Ea"
   },
   "outputs": [],
   "source": [
    "#Tanuja Baliram Kaklij\n",
    "#Assignment No.: 07\n",
    "#Title: Text Analytics\n",
    "#1. Extract Sample document and apply following document preprocessing methods:\n",
    "#Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "#2. Create representation of documents by calculating Term Frequency and Inverse\n",
    "#DocumentFrequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwgcZYLleUqh",
    "outputId": "22433927-ed67-487f-853e-7477e9b5a208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'barked', 'in', 'response', 'but', 'the', 'fox', 'was', 'already', 'gone', '.', 'This', 'was', 'the', 'third', 'time', 'the', 'fox', 'had', 'visited', 'the', 'garden', ',', 'and', 'the', 'dog', 'was', 'determined', 'to', 'catch', 'him', 'next', 'time', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Tokenization: This involves breaking up the text into individual tokens,\n",
    "#which are typically words or punctuation marks.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "document = \"The quick brown fox jumped over the lazy dog. The dog barked in response but the fox was already gone. This was the third time the fox had visited the garden, and the dog was determined to catch him next time.\"\n",
    "\n",
    "tokens = word_tokenize(document)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7hgIpzreZDl",
    "outputId": "99ade5ff-f96e-4636-b70e-422a7d98d164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('The', 'DT'), ('dog', 'NN'), ('barked', 'VBD'), ('in', 'IN'), ('response', 'NN'), ('but', 'CC'), ('the', 'DT'), ('fox', 'NN'), ('was', 'VBD'), ('already', 'RB'), ('gone', 'VBN'), ('.', '.'), ('This', 'DT'), ('was', 'VBD'), ('the', 'DT'), ('third', 'JJ'), ('time', 'NN'), ('the', 'DT'), ('fox', 'NN'), ('had', 'VBD'), ('visited', 'VBN'), ('the', 'DT'), ('garden', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('dog', 'NN'), ('was', 'VBD'), ('determined', 'VBN'), ('to', 'TO'), ('catch', 'VB'), ('him', 'PRP'), ('next', 'JJ'), ('time', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#POS Tagging: This involves assigning a part-of-speech tag to each token, such as noun, verb, adjective, etc.\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbWL1tCMfDgN",
    "outputId": "f2725821-e525-48e1-be9a-fe2bfef01029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', '.', 'dog', 'barked', 'response', 'fox', 'already', 'gone', '.', 'third', 'time', 'fox', 'visited', 'garden', ',', 'dog', 'determined', 'catch', 'next', 'time', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Stop Words Removal: This involves removing commonly occurring words such as \"the\", \"and\", \"a\", etc. \n",
    "#that do not contribute much to the meaning of the text.\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caysupltfNtf",
    "outputId": "cf29281d-d3c8-44f7-caa5-53a3181dde76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', '.', 'dog', 'bark', 'respons', 'fox', 'alreadi', 'gone', '.', 'third', 'time', 'fox', 'visit', 'garden', ',', 'dog', 'determin', 'catch', 'next', 'time', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming: This involves reducing words to their base or root form.\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZxsTZt1fs5s",
    "outputId": "4ac69174-45a7-4dc6-c990-6820f5edf942"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', '.', 'dog', 'barked', 'response', 'fox', 'already', 'gone', '.', 'third', 'time', 'fox', 'visited', 'garden', ',', 'dog', 'determined', 'catch', 'next', 'time', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization: This involves reducing words to their base or dictionary form, which is called a lemma. \n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzXVO5jMf2kA",
    "outputId": "1c7ba5b8-36e4-4b2b-ab23-2cad1eadc833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'new': 2, 'york': 5, 'times': 4, 'post': 3, 'los': 1, 'angles': 0}\n",
      "\n",
      "tf-idf value:\n",
      "  (0, 4)\t0.5773502691896257\n",
      "  (0, 5)\t0.5773502691896257\n",
      "  (0, 2)\t0.5773502691896257\n",
      "  (1, 3)\t0.680918560398684\n",
      "  (1, 5)\t0.5178561161676974\n",
      "  (1, 2)\t0.5178561161676974\n",
      "  (2, 0)\t0.6227660078332259\n",
      "  (2, 1)\t0.6227660078332259\n",
      "  (2, 4)\t0.4736296010332684\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.         0.         0.57735027 0.         0.57735027 0.57735027]\n",
      " [0.         0.         0.51785612 0.68091856 0.         0.51785612]\n",
      " [0.62276601 0.62276601 0.         0.         0.4736296  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Term Frequency (TF) measures the frequency of a term in a document.\n",
    "#Inverse Document Frequency (IDF) measures the rarity of a term across all documents.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "d0 = 'new york times'\n",
    "d1 = 'new york post'\n",
    "d2 = 'los angles times'\n",
    "series = [d0, d1, d2]\n",
    "#create object\n",
    "tfidf = TfidfVectorizer()\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(series)\n",
    "#get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zdVZSWvgBCA",
    "outputId": "a4df9d30-5f23-4027-d2b9-bf1e7b83a12d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Document Frequency:  {'q': 4.6491870714048655, 'u': 4.243721963296702, 'i': 3.396424102909498, 'c': 4.243721963296702, 'k': 4.243721963296702, 'b': 4.243721963296702, 'r': 3.262892710284975, 'o': 3.039749158970765, 'w': 4.6491870714048655, 'n': 3.396424102909498, 'f': 3.9560398908449206, 'x': 3.7328963395307104, 'j': 4.6491870714048655, 'm': 3.7328963395307104, 'p': 4.243721963296702, 'e': 2.8574276021768106, 'd': 2.9444389791664403, 'l': 4.243721963296702, 'a': 3.550574782736756, 'z': 4.6491870714048655, 'y': 4.243721963296702, 'g': 3.550574782736756, '.': 3.9560398908449206, 's': 4.243721963296702, 't': 3.262892710284975, 'h': 4.243721963296702, 'v': 4.6491870714048655, ',': 4.6491870714048655}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "idf = {}\n",
    "for tf in tf_list:\n",
    "    for token in tf:\n",
    "        if token in idf:\n",
    "            idf[token] += 1\n",
    "        else:\n",
    "            idf[token] = 1\n",
    "for token in idf:\n",
    "    idf[token] = math.log(len(document) / (idf[token] + 1))\n",
    "print(\"Inverse Document Frequency: \", idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7GaBFbfgp3o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
